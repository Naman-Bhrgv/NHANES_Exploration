{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bp_8VKmdW5q3",
    "outputId": "c9272367-7910-47fe-c56a-f7534c192ec5"
   },
   "outputs": [],
   "source": [
    "pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0AAvo6FLYJJ",
    "outputId": "4a0fadd1-23d3-4934-d518-8beb402d6ab0"
   },
   "outputs": [],
   "source": [
    "pip install pygam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import xgboost\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from  sklearn.feature_selection import SelectFromModel\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from pygam import LinearGAM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, KFold,\n",
    "                                     GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from  sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ofg9dAorlq-L",
    "outputId": "4a131195-e1a6-4f77-e6b2-0c81cf45aaf4"
   },
   "outputs": [],
   "source": [
    "## Task 1 Data Prep\n",
    "\n",
    "directory = \"Data\\Task_1_Data\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "\n",
    "# Filter files with extension .XPT\n",
    "xpt_files = [file for file in files if file.endswith(\".XPT\")]\n",
    "\n",
    "df_arr=[]\n",
    "\n",
    "# Iterate over each XPT file\n",
    "for file in xpt_files:\n",
    "\n",
    "    file_path = os.path.join(directory, file)\n",
    "\n",
    "    # Load the XPT file\n",
    "    df, meta = pyreadstat.read_xport(file_path)\n",
    "\n",
    "    # Process the data as needed\n",
    "    print(f\"Loaded file: {file}\")\n",
    "    print(df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "    df_arr.append(df)\n",
    "#xpt_files\n",
    "for i in df_arr:\n",
    "\n",
    "    print(i.shape)\n",
    "df_concat=df_arr[0].merge(df_arr[1],on='SEQN')\n",
    "\n",
    "print(\"Merge 1:\")\n",
    "\n",
    "print(df_arr[0].shape)\n",
    "\n",
    "print(df_arr[1].shape)\n",
    "\n",
    "print(df_concat.shape)\n",
    "\n",
    "c=0\n",
    "\n",
    "df_sub=df_arr[2:]\n",
    "\n",
    "for i in df_sub:\n",
    "\n",
    "\n",
    "\n",
    "    print(c,end=\" \")\n",
    "\n",
    "    print(\":\")\n",
    "\n",
    "    print(df_concat.shape)\n",
    "\n",
    "    print(i.shape)\n",
    "\n",
    "    df_concat=df_concat.merge(i,on='SEQN')\n",
    "\n",
    "    print(df_concat.shape)\n",
    "\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuP-R9IZmENf",
    "outputId": "0dd3fbac-62b3-4f96-b070-c02440d846fd"
   },
   "outputs": [],
   "source": [
    "len(df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "834aKPHAW1LX",
    "outputId": "e2135fb9-7864-4374-f63c-b850e9683ad9"
   },
   "outputs": [],
   "source": [
    "for i in df_arr:\n",
    "\n",
    "    print(i.columns)\n",
    "l=[1,2,3,4,5]\n",
    "\n",
    "print(l[2:])\n",
    "l=[]\n",
    "\n",
    "for i in df_concat.isnull().sum():\n",
    "\n",
    "    l.append(i/df_concat.shape[0])\n",
    "\n",
    "l_col=list(df_concat.isnull().sum().index)\n",
    "d={'0':0,'0.1':0,'0.2':0,'0.3':0,'0.4':0,'0.5':0,'0.6':0,'0.7':0,'0.8':0,'0.9':0,'1':0}\n",
    "\n",
    "d_colname={'0':[],'0.1':[],'0.2':[],'0.3':[],'0.4':[],'0.5':[],'0.6':[],'0.7':[],'0.8':[],'0.9':[],'1':[]}\n",
    "\n",
    "c=0\n",
    "for i in l:\n",
    "\n",
    "    if i<0.1:\n",
    "\n",
    "        d['0']+=1\n",
    "\n",
    "        d_colname['0'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.1 and i<0.2:\n",
    "\n",
    "        d['0.1']+=1\n",
    "\n",
    "        d_colname['0.1'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.2 and i<0.3:\n",
    "\n",
    "        d['0.2']+=1\n",
    "        d_colname['0.2'].append(l_col[c])\n",
    "    elif i>=0.3 and i<0.4:\n",
    "\n",
    "        d['0.3']+=1\n",
    "        d_colname['0.3'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.4 and i<0.5:\n",
    "\n",
    "        d['0.4']+=1\n",
    "        d_colname['0.4'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.5 and i<0.6:\n",
    "\n",
    "        d['0.5']+=1\n",
    "        d_colname['0.5'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.6 and i<0.7:\n",
    "\n",
    "        d['0.6']+=1\n",
    "        d_colname['0.6'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.7 and i<0.8:\n",
    "\n",
    "        d['0.7']+=1\n",
    "        d_colname['0.7'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.8 and i<0.9:\n",
    "\n",
    "        d['0.8']+=1\n",
    "        d_colname['0.8'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.9 and i<1:\n",
    "\n",
    "        d['0.9']+=1\n",
    "        d_colname['0.9'].append(l_col[c])\n",
    "\n",
    "    else:\n",
    "        d['1']+=1\n",
    "        d_colname['1'].append(l_col[c])\n",
    "\n",
    "    c+=1\n",
    "\n",
    "df_concat=df_concat.drop(d_colname['0.8'],axis=1)\n",
    "\n",
    "df_concat=df_concat.drop(d_colname['0.9'],axis=1)\n",
    "\n",
    "df_concat=df_concat.drop(d_colname['1'],axis=1)\n",
    "\n",
    "df_concat = df_concat[df_concat['BMXBMI'].notna()]\n",
    "\n",
    "#print(df_concat.shape)\n",
    "y=df_concat['BMXBMI']\n",
    "\n",
    "X=df_concat.drop('BMXBMI', axis=1)\n",
    "\n",
    "X_col=list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMpP97G_nYGp",
    "outputId": "1fdb0c40-cf87-4d39-aacf-26e7acf41fdc"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozymhjUpmcSG"
   },
   "outputs": [],
   "source": [
    "#Data Imputation with Median\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit the imputer on the data\n",
    "imputer.fit(X)\n",
    "\n",
    "# Impute the missing values\n",
    "imputed_data = imputer.transform(X)\n",
    "\n",
    "\n",
    "imputed_data = pd.DataFrame(imputed_data, columns=X_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9W2Wtwpnqzm"
   },
   "outputs": [],
   "source": [
    "#Data Imputation with knn\n",
    "X_col=list(X.columns)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "df_concat_impute_knn = imputer.fit_transform(X)\n",
    "\n",
    "imputed_df_knn = pd.DataFrame(df_concat_impute_knn, columns=X_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0mBRTjoOLGp",
    "outputId": "7e37cee0-56f9-4df3-ebc9-549cd3dfe23b"
   },
   "outputs": [],
   "source": [
    "#Compare performance of imputation methods\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "y_pred=ridge_model.predict(X_test_scaled)\n",
    "print(\"Model performance on Data imputed using Median:\")\n",
    "print(r2_score(y_pred,y_test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_df_knn, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "y_pred=ridge_model.predict(X_test_scaled)\n",
    "print(\"Model performance on Data imputed using KNN:\")\n",
    "print(r2_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJtHj9U2n8nE",
    "outputId": "48b8391b-50d9-4fe2-e50d-055498d8df9f"
   },
   "outputs": [],
   "source": [
    "#SelectKBest Feature Selection\n",
    "\n",
    "def k_best_feat_sel(df,y):\n",
    "\n",
    "  sel=SelectKBest(score_func= f_regression,k=50)\n",
    "\n",
    "  X_feat_sel=sel.fit_transform(df,y)\n",
    "\n",
    "\n",
    "  selected_indices = sel.get_support(indices=True)\n",
    "\n",
    "  #print(selected_indices)\n",
    "  # Get the column names of the selected features\n",
    "  X_feat_sel_df = df.iloc[:,selected_indices]\n",
    "\n",
    "  return X_feat_sel_df\n",
    "\n",
    "\n",
    "#imputed_df_xgb.to_csv(\"t1_all_feat_xgb.csv\")\n",
    "\n",
    "#X_feat_sel_df.to_csv(\"t1_sel_feat_xgb.csv\")\n",
    "\n",
    "#y.to_csv(\"t1_tgt.csv\")\n",
    "\n",
    "#X_feat_sel_xgb=k_best_feat_sel(imputed_df_xgb)\n",
    "\n",
    "X_feat_sel_knn=k_best_feat_sel(imputed_df_knn,y)\n",
    "\n",
    "\n",
    "print(X_feat_sel_knn)\n",
    "\n",
    "y_t1=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQpdL7-TStXJ",
    "outputId": "bbf41189-7e7a-4483-839d-ffefcc86e429"
   },
   "outputs": [],
   "source": [
    "#Task 2\n",
    "#Data Prep\n",
    "\n",
    "directory = 'Data\\Task_2_Data'\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "\n",
    "# Filter files with extension .XPT\n",
    "xpt_files = [file for file in files if file.endswith(\".XPT\")]\n",
    "\n",
    "df_arr=[]\n",
    "\n",
    "# Iterate over each XPT file\n",
    "for file in xpt_files:\n",
    "\n",
    "    file_path = os.path.join(directory, file)\n",
    "\n",
    "    # Load the XPT file\n",
    "    df, meta = pyreadstat.read_xport(file_path)\n",
    "\n",
    "    # Process the data as needed\n",
    "    print(f\"Loaded file: {file}\")\n",
    "\n",
    "    print(df.columns)\n",
    "    #print(df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "    df_arr.append(df)\n",
    "\n",
    "df_arr[1]=df_arr[1][['SEQN', 'BPQ020']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJnGY9iiUDDA",
    "outputId": "eba10d7d-7fc7-409f-b5b0-f523cdd4ced2"
   },
   "outputs": [],
   "source": [
    "df_concat=df_arr[0].merge(df_arr[1],on='SEQN')\n",
    "\n",
    "#print(df_arr[0].shape)\n",
    "\n",
    "#print(df_arr[1].shape)\n",
    "\n",
    "for i in range(2,len(df_arr)):\n",
    "\n",
    "  df_concat=df_concat.merge(df_arr[i],on='SEQN')\n",
    "\n",
    "print(df_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6N_NNDYiXPQu",
    "outputId": "71074508-4aa0-4204-d424-a95913ddfe54"
   },
   "outputs": [],
   "source": [
    "#Check Missing Values\n",
    "\n",
    "l=[]\n",
    "\n",
    "for i in df_concat.isnull().sum():\n",
    "\n",
    "    l.append(i/df_concat.shape[0])\n",
    "\n",
    "l_col=list(df_concat.isnull().sum().index)\n",
    "d={'0':0,'0.1':0,'0.2':0,'0.3':0,'0.4':0,'0.5':0,'0.6':0,'0.7':0,'0.8':0,'0.9':0,'1':0}\n",
    "\n",
    "d_colname={'0':[],'0.1':[],'0.2':[],'0.3':[],'0.4':[],'0.5':[],'0.6':[],'0.7':[],'0.8':[],'0.9':[],'1':[]}\n",
    "\n",
    "c=0\n",
    "for i in l:\n",
    "\n",
    "    if i<0.1:\n",
    "\n",
    "        d['0']+=1\n",
    "\n",
    "        d_colname['0'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.1 and i<0.2:\n",
    "\n",
    "        d['0.1']+=1\n",
    "\n",
    "        d_colname['0.1'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.2 and i<0.3:\n",
    "\n",
    "        d['0.2']+=1\n",
    "        d_colname['0.2'].append(l_col[c])\n",
    "    elif i>=0.3 and i<0.4:\n",
    "\n",
    "        d['0.3']+=1\n",
    "        d_colname['0.3'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.4 and i<0.5:\n",
    "\n",
    "        d['0.4']+=1\n",
    "        d_colname['0.4'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.5 and i<0.6:\n",
    "\n",
    "        d['0.5']+=1\n",
    "        d_colname['0.5'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.6 and i<0.7:\n",
    "\n",
    "        d['0.6']+=1\n",
    "        d_colname['0.6'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.7 and i<0.8:\n",
    "\n",
    "        d['0.7']+=1\n",
    "        d_colname['0.7'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.8 and i<0.9:\n",
    "\n",
    "        d['0.8']+=1\n",
    "        d_colname['0.8'].append(l_col[c])\n",
    "\n",
    "    elif i>=0.9 and i<1:\n",
    "\n",
    "        d['0.9']+=1\n",
    "        d_colname['0.9'].append(l_col[c])\n",
    "\n",
    "    else:\n",
    "        d['1']+=1\n",
    "        d_colname['1'].append(l_col[c])\n",
    "\n",
    "    c+=1\n",
    "\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nu3n8FtsVfIj",
    "outputId": "90b74320-f0d9-4924-941e-6d85a891e9e9"
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat[df_concat['BPQ020'].notna()]\n",
    "\n",
    "print(df_concat.shape)\n",
    "y=df_concat['BPQ020']\n",
    "\n",
    "X=df_concat.drop('BPQ020', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEghu5g5XfPv"
   },
   "outputs": [],
   "source": [
    "X=X.drop(d_colname['1'], axis=1)\n",
    "\n",
    "X=X.drop(d_colname['0.9'], axis=1)\n",
    "\n",
    "X=X.drop(d_colname['0.8'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6jIOHiGN2fe",
    "outputId": "2e72d584-dc7d-4594-985c-1af1b9e2ed82"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfBsjO-GcKOF",
    "outputId": "a355e0f1-8e58-4a89-f144-15bfe33c04fe"
   },
   "outputs": [],
   "source": [
    "for i in X.columns:\n",
    "\n",
    "    if X[i].dtype!='float64':\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        print(X[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_5goh0jcyjL"
   },
   "outputs": [],
   "source": [
    "X=X.drop('DSDSUPP', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "4py6-QOWYDM5",
    "outputId": "cb1fea5d-e197-48d7-c024-32bcffb69bdd"
   },
   "outputs": [],
   "source": [
    "#Data Imputation with Median\n",
    "\n",
    "X_col=list(X.columns)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit the imputer on the data\n",
    "imputer.fit(X)\n",
    "\n",
    "# Impute the missing values\n",
    "imputed_data_t2 = imputer.transform(X)\n",
    "\n",
    "\n",
    "imputed_data_t2 = pd.DataFrame(imputed_data_t2, columns=X_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "DqH5jhnvpTXA",
    "outputId": "2b3ebe78-e3e0-41d0-d092-1505601416d0"
   },
   "outputs": [],
   "source": [
    "#Data Imputation with knn\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "df_concat_impute_t2 = imputer.fit_transform(X)\n",
    "\n",
    "X_col=list(X.columns)\n",
    "imputed_df_knn_t2 = pd.DataFrame(df_concat_impute_t2, columns=X_col)\n",
    "#imputed_df_knn_t2['tgt']=y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare performance of missing data imputation methods on AdaBoost\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_df_knn_t2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ada_model = AdaBoostClassifier()\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "y_pred=ada_model.predict(X_test_scaled)\n",
    "print(\"Adboost performance of data imputed with KNN:\")\n",
    "print(accuracy_score(y_pred,y_test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_data_t2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ada_model = AdaBoostClassifier()\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "y_pred=ada_model.predict(X_test_scaled)\n",
    "print(\"Adboost performance of data imputed with median:\")\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nrH5kTZYQbg"
   },
   "outputs": [],
   "source": [
    "#Handling Confounding Variables\n",
    "\n",
    "#Identifying it using VIF\n",
    "\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = imputed_df_knn_t2.columns\n",
    "\n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(imputed_df_knn_t2.values, i) for i in range(len(imputed_df_knn_t2.columns))]\n",
    "\n",
    "#Lasso to handle confounding variables.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = 0.1  # regularization strength\n",
    "lasso = Lasso(alpha=alpha)\n",
    "lasso.fit(imputed_df_knn_t2, y)\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients:\", lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "URQrj6n2VEht",
    "outputId": "f242540c-9b10-4481-d2d2-bf6b283a3dd0"
   },
   "outputs": [],
   "source": [
    "#EDA\n",
    "#V1\n",
    "df_concat['BPQ020'].value_counts().plot.pie(shadow=True, title=\"Distribution of Hypertension among Respondents\", legend=True)\n",
    "# Creating a custom legend with desired labels\n",
    "custom_labels = ['No', 'Yes', 'Don\\'t Know']\n",
    "plt.legend(custom_labels,loc='best', bbox_to_anchor=(1, 0., 0.5, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "rhKdRQ84lAV1",
    "outputId": "0a718337-5cf5-4d50-9bfa-7904206b89b2"
   },
   "outputs": [],
   "source": [
    "#V2\n",
    "\n",
    "\n",
    "sns.histplot(data=df_concat, x='RIDAGEYR', hue='BPQ020', palette=[\"#FF5733\", \"#33FF57\", \"#3366FF\"])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.title('Distribution of Hypertension v/s Respondent age')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvtI03EIue1Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cross_tab = pd.crosstab(df_concat['FSDHH'], df_concat['BPQ020'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "XKBKW13ws_9q",
    "outputId": "7497130d-fd2c-4078-89f7-f8e3938c16a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cross_tab, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title('Investigating impact of Food Security on Hypertension')\n",
    "plt.xlabel(\"Hypertension\")\n",
    "plt.ylabel(\"Food Security\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR-zQ1FO8cDy"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvORSy5rJPRp"
   },
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N8UaijmmJj-f",
    "outputId": "15e50a0e-5a04-4493-b134-1715db4caafe"
   },
   "outputs": [],
   "source": [
    "def t1_model(X,y):\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "  ridge_model = Ridge()\n",
    "  ridge_model.fit(X_train, y_train)\n",
    "\n",
    "  gam_model = LinearGAM().fit(X_train, y_train)\n",
    "\n",
    "  models = [ridge_model, gam_model]\n",
    "  model_names = ['Ridge','GAM']\n",
    "  metrics = [mean_absolute_error, r2_score, explained_variance_score]\n",
    "\n",
    "  for model, name in zip(models, model_names):\n",
    "      print(\"Model:\", name)\n",
    "      y_pred = model.predict(X_test)\n",
    "      for metric in metrics:\n",
    "          score = metric(y_test, y_pred)\n",
    "          print(metric.__name__, \":\", score)\n",
    "      print(\"\\n\")\n",
    "  def plot_predicted_vs_actual(y_test, y_pred, model_name):\n",
    "    correlation_coef, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    plt.xlabel('Actual BMI')\n",
    "    plt.ylabel('Predicted BMI')\n",
    "    plt.title(f'{model_name} Predicted vs. Actual BMI (Correlation: {correlation_coef:.2f})')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "  plot_predicted_vs_actual(y_test, y_pred, 'Ridge')\n",
    "\n",
    "  n_splits = 5\n",
    "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "  cv_scores_ridge = []\n",
    "  for train_index, test_index in kf.split(X):\n",
    "      X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "      y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      ridge_model_cv = Ridge()\n",
    "      ridge_model_cv.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "      y_pred_cv = ridge_model_cv.predict(X_test_cv)\n",
    "      cv_score = mean_absolute_error(y_test_cv, y_pred_cv)\n",
    "      cv_scores_ridge.append(cv_score)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.boxplot(cv_scores_ridge)\n",
    "  plt.title('Cross-Validation Results for Ridge Model')\n",
    "  plt.xlabel('Fold')\n",
    "  plt.ylabel('Mean Absolute Error')\n",
    "  plt.show()\n",
    "\n",
    "  n_splits = 5\n",
    "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "  cv_scores_gam = []\n",
    "  for train_index, test_index in kf.split(X):\n",
    "      X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "      y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      gam_model_cv = LinearGAM().fit(X_train_cv, y_train_cv)\n",
    "\n",
    "      y_pred_cv = gam_model_cv.predict(X_test_cv)\n",
    "      cv_score = mean_absolute_error(y_test_cv, y_pred_cv)\n",
    "      cv_scores_gam.append(cv_score)\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.boxplot(cv_scores_gam)\n",
    "  plt.title('Cross-Validation Results for GAM Model')\n",
    "  plt.xlabel('Fold')\n",
    "  plt.ylabel('Mean Absolute Error')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "t1_model(X_feat_sel_knn,y_t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbWRE2y0DgJ2",
    "outputId": "aba12d63-2550-4080-e860-33bba605767a"
   },
   "outputs": [],
   "source": [
    "#Task 2\n",
    "\n",
    "#X_all_rf = t2_all_rf.drop(['Unnamed: 0', 'SEQN'], axis=1)\n",
    "#y = t2_tgt['BPQ020']\n",
    "\n",
    "def t2_model(X_all_rf,y):\n",
    "\n",
    "\n",
    "    X_train_all_rf, X_test_all_rf, y_train, y_test = train_test_split(X_all_rf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    param_grids = {\n",
    "        'AdaBoost': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'learning_rate': [0.1, 0.01, 0.001]\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 150],\n",
    "            'max_depth': [20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def tune_train_model(model, param_distributions, X_train, y_train, kf, n_iter=10):\n",
    "        random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions,\n",
    "                                           n_iter=n_iter, scoring='accuracy', cv=kf,\n",
    "                                           n_jobs=-1, random_state=42)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        return random_search.best_estimator_\n",
    "\n",
    "    ada_best_model = tune_train_model(AdaBoostClassifier(), param_grids['AdaBoost'],\n",
    "                                      X_train_all_rf, y_train, kf, n_iter=5)\n",
    "\n",
    "    rf_best_model = tune_train_model(RandomForestClassifier(), param_grids['RandomForest'],\n",
    "                                     X_train_all_rf, y_train, kf, n_iter=5)\n",
    "\n",
    "    ada_pred_all = ada_best_model.predict(X_test_all_rf)\n",
    "    ada_accuracy_all = accuracy_score(y_test, ada_pred_all)\n",
    "    print(\"AdaBoost Best Model Accuracy:\", ada_accuracy_all)\n",
    "\n",
    "    rf_pred_all = rf_best_model.predict(X_test_all_rf)\n",
    "    rf_accuracy_all = accuracy_score(y_test, rf_pred_all)\n",
    "    print(\"Random Forest Best Model Accuracy:\", rf_accuracy_all)\n",
    "\n",
    "    ada_cv_scores = cross_val_score(ada_best_model, X_all_rf, y, cv=kf)\n",
    "    rf_cv_scores = cross_val_score(rf_best_model, X_all_rf, y, cv=kf)\n",
    "    print(\"AdaBoost Best Model Cross-Validation Scores:\", ada_cv_scores.mean())\n",
    "    print(\"Random Forest Best Model Cross-Validation Scores:\", rf_cv_scores.mean())\n",
    "\n",
    "    voting_clf_hard = VotingClassifier(\n",
    "        estimators=[('AdaBoost', ada_best_model), ('RandomForest', rf_best_model)],\n",
    "        voting='hard'\n",
    "    )\n",
    "\n",
    "    voting_clf_hard.fit(X_train_all_rf, y_train)\n",
    "\n",
    "    voting_pred_hard = voting_clf_hard.predict(X_test_all_rf)\n",
    "\n",
    "    voting_accuracy_hard = accuracy_score(y_test, voting_pred_hard)\n",
    "    print(\"Voting Classifier (Hard) Accuracy:\", voting_accuracy_hard)\n",
    "\n",
    "    ada_model_with_proba = AdaBoostClassifier()\n",
    "\n",
    "    voting_clf_soft = VotingClassifier(\n",
    "        estimators=[('AdaBoost', ada_model_with_proba), ('RandomForest', rf_best_model)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    voting_clf_soft.fit(X_train_all_rf, y_train)\n",
    "\n",
    "    voting_pred_soft = voting_clf_soft.predict(X_test_all_rf)\n",
    "\n",
    "    voting_accuracy_soft = accuracy_score(y_test, voting_pred_soft)\n",
    "    print(\"Voting Classifier (Soft) Accuracy:\", voting_accuracy_soft)\n",
    "\n",
    "\n",
    "    ada_cm = confusion_matrix(y_test, ada_pred_all)\n",
    "\n",
    "    rf_cm = confusion_matrix(y_test, rf_pred_all)\n",
    "\n",
    "    print(\"Confusion Matrix for AdaBoost:\")\n",
    "    print(ada_cm)\n",
    "    print(\"\\nConfusion Matrix for Random Forest:\")\n",
    "    print(rf_cm)\n",
    "\n",
    "    ada_classification_report = classification_report(y_test, ada_pred_all)\n",
    "    rf_classification_report = classification_report(y_test, rf_pred_all)\n",
    "\n",
    "    print(\"\\nClassification Report for AdaBoost:\")\n",
    "    print(ada_classification_report)\n",
    "    print(\"\\nClassification Report for Random Forest:\")\n",
    "    print(rf_classification_report)\n",
    "\n",
    "    ada_roc_auc_scores = []\n",
    "\n",
    "    for i in range(len(ada_best_model.classes_)):\n",
    "        y_test_binary = (y_test == ada_best_model.classes_[i]).astype(int)\n",
    "        ada_roc_auc_scores.append(roc_auc_score(y_test_binary, ada_best_model.predict_proba(X_test_all_rf)[:, i]))\n",
    "\n",
    "    ada_micro_auc_score = roc_auc_score(y_test, ada_best_model.predict_proba(X_test_all_rf), multi_class='ovr', average='micro')\n",
    "\n",
    "    print(\"ROC AUC Scores for AdaBoost (One-vs-Rest):\", ada_roc_auc_scores)\n",
    "    print(\"Micro-average ROC AUC Score for AdaBoost:\", ada_micro_auc_score)\n",
    "\n",
    "\n",
    "    def plot_roc_curves(y_test, y_probs, model_name, classes):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(len(classes)):\n",
    "            fpr, tpr, thresholds = roc_curve((y_test == classes[i]), y_probs[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'Class {classes[i]} (AUC = {roc_auc_score((y_test == classes[i]), y_probs[:, i]):.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {model_name}')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    ada_probs = ada_best_model.predict_proba(X_test_all_rf)\n",
    "    plot_roc_curves(y_test, ada_probs, 'AdaBoost', ada_best_model.classes_)\n",
    "\n",
    "    rf_probs = rf_best_model.predict_proba(X_test_all_rf)\n",
    "    plot_roc_curves(y_test, rf_probs, 'RandomForest', rf_best_model.classes_)\n",
    "\n",
    "    ada_cm = confusion_matrix(y_test, ada_pred_all)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(ada_cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix - AdaBoost')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    rf_cm = confusion_matrix(y_test, rf_pred_all)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(rf_cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix - Random Forest')\n",
    "    plt.show()\n",
    "\n",
    "t2_model(imputed_df_knn_t2,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
